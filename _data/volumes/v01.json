[
    {
        "abstract": "The increasing demand for high-quality datasets in machine learning has raised concerns about the ethical and responsible creation of these datasets. Dataset creators play a crucial role in developing responsible practices, yet their perspectives and expertise have not yet been highlighted in the current literature. In this paper, we bridge this gap by presenting insights from a qualitative study that included interviewing 18 leading dataset creators about the current state of the field. We shed light on the challenges and considerations faced by dataset creators, and our findings underscore the potential for deeper collaboration, knowledge sharing, and collective development. Through a close analysis of their perspectives, we share seven central recommendations for improving responsible dataset creation, including issues such as data quality, documentation, privacy and consent, and how to mitigate potential harms from unintended use cases. By fostering critical reflection and sharing the experiences of dataset creators, we aim to promote responsible dataset creation practices and develop a nuanced understanding of this crucial but often undervalued aspect of machine learning research.",
        "authors": [
            "Will Orr",
            "Kate Crawford"
        ],
        "title": "Building Better Datasets: Seven Recommendations for Responsible Design from Dataset Creators",
        "volume": "01",
        "id": "1",
        "year": "2024",
        "page":"21"
    },
    {
        "abstract": "Multimodal image-text models have shown remarkable performance in the past few years.  However, evaluating robustness against distribution shifts is crucial before adopting them in real-world applications. In this work, we investigate the robustness of 12 popular open-sourced image-text models under common perturbations on five tasks (image-text retrieval, visual reasoning, visual entailment, image captioning, and text-to-image generation). In particular, we propose several new multimodal robustness benchmarks by applying 17 image perturbation and 16 text perturbation techniques on top of existing datasets.  We observe that multimodal models are not robust to image and text perturbations, especially to image perturbations. Among the tested perturbation methods, character-level perturbations constitute the most severe distribution shift for text, and zoom blur is the most severe shift for image data.  We also introduce two new robustness metrics (*MMI* for MultiModal Impact score and *MOR* for Missing Object Rate) for proper evaluations of multimodal models. We hope our extensive study sheds light on new directions for the development of robust multimodal models.  More details can be found on the project webpage: <a href='https://MMRobustness.github.io'>https://MMRobustness.github.io</a>.",
        "authors": [
            "Jielin Qiu",
            "Yi Zhu",
            "Xingjian Shi",
            "Florian Wenzel",
            "Zhiqiang Tang",
            "Ding Zhao",
            "Bo Li",
            "Mu Li"
        ],
        "title": "Benchmarking Robustness of Multimodal Image-Text Models under Distribution Shift",
        "volume": "01",
        "id": "2",
        "page": "56",
        "year": "2024"
    },
    {
        "abstract": "This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary or errors between the target values and an SR model’s predicted values for a given input. We conduct benchmark experiments on our new SRSD datasets using various representative SR methods. The experimental results show that we provide a more realistic performance evaluation, and our user study shows that the NED correlates with human judges significantly more than an existing SR metric. We publish repositories of our code and 240 SRSD datasets.",
        "authors": [
            "Yoshitomo Matsubara", "Naoya Chiba", "Ryo Igarashi", "Yoshitaka Ushiku"
        ],
        "title": "Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery",
        "volume": "01",
        "id": "3",
        "page": "38",
        "year": "2024"
    },
    {
        "abstract": "ImageNet is the most cited and well-known dataset for training image classification models. The people categories of its original version from 2009 have been found to be highly problematic (e.g. Crawford and Paglen (2019); Prabhu and Birhane (2020)) and have since been updated to improve their representativity (Yang et al., 2020). In this paper, we examine the past and present versions of the dataset from a variety of quantitative and qualitative angles and note several technical, epistemological and institutional issues, including duplicates, erroneous images, dehumanizing content, and lack of consent. We also discuss the concepts of ‘safety’ and ‘imageability’, which were established as criteria for filtering the people categories of the most recent version of ImageNet 21K. We conclude with a discussion of automated essentialism, the fundamental ethical problem that arises when datasets categorize human identity into a set number of discrete categories based on visual characteristics alone. We end with a call upon the ML community to reassess how training datasets that include human subjects are created and used.",
        "authors": [
            "Sasha Luccioni", "Kate Crawford"
        ],
        "title": "The Nine Lives of ImageNet: A Sociotechnical Retrospective of a Foundation Dataset and the Limits of Automated Essentialism",
        "volume": "01",
        "id": "4",
        "page": "18",
        "year": "2024"
    },
    {
        "abstract": "Drawing from discussions at the inaugural DMLR workshop at ICML 2023 and meetings prior, in this report we outline the relevance of community engagement and infrastructure development for the creation of next-generation public datasets that will advance machine learning science. We chart a path forward as a collective effort to sustain the creation and maintenance of these datasets and methods towards positive scientific, societal and business impact.",
        "authors": [
            "Luis Oala", "Manil Maskey", "Lilith Bat-Leah", "Alicia Parrish", "Nezihe Merve Gürel", "Tzu-Sheng Kuo", "Yang Liu", "Rotem Dror", "Danilo Brajovic", "Xiaozhe Yao", "Max Bartolo", "William A Gaviria Rojas", "Ryan Hileman", "Rainier Aliment", "Michael W. Mahoney", "Meg Risdal", "Matthew Lease", "Wojciech Samek", "Debojyoti Dutta", "Curtis G Northcutt", "Cody Coleman", "Braden Hancock", "Bernard Koch", "Girmaw Abebe Tadesse", "Bojan Karlaš", "Ahmed Alaa", "Adji Bousso Dieng", "Natasha Noy", "Vijay Janapa Reddi", "James Zou", "Praveen Paritosh", "Mihaela van der Schaar", "Kurt Bollacker", "Lora Aroyo", "Ce Zhang", "Joaquin Vanschoren", "Isabelle Guyon", "Peter Mattson"
        ],
        "title": "DMLR: Data-centric Machine Learning Research - Past, Present and Future",
        "volume": "01",
        "id": "5",
        "page": "27",
        "year": "2024"
    },
    {
        "abstract": "Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. We consider general regression settings with covariates and a potentially corrupted response whose observed values may contain errors. By accounting for various uncertainties, we introduced veracity scores that distinguish between genuine errors and natural data fluctuations, conditioned on the available covariate information in the dataset. We propose a simple yet efficient filtering procedure for eliminating potential errors, and establish theoretical guarantees for our method. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are also known). In this benchmark and additional simulation studies, our method identifies incorrect values with better precision/recall than other approaches.",
        "authors": [
            "Hang Zhou", "Jonas Mueller", "Mayank Kumar", "Jane-Ling Wang", "Jing Lei"
        ],
        "title": "Detecting Errors in a Numerical Response via any Regression Model",
        "volume": "01",
        "id": "6",
        "page": "25",
        "year": "2024"
    },
    {
        "abstract": "Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be *label-efficient*: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates significantly better label-efficiencies than previously reported in active learning. LabelBench's modular codebase is open-sourced for the broader community to contribute label-efficient learning methods and benchmarks. The repository can be found at: [https://github.com/EfficientTraining/LabelBench](https://github.com/EfficientTraining/LabelBench).",
        "authors": [
            "Jifan Zhang", "Yifang Chen", "Gregory Canal", "Arnav Mohanty Das", "Gantavya Bhatt", "Stephen Mussmann", "Yinglun Zhu", "Jeff Bilmes", "Simon Shaolei Du", "Kevin Jamieson", "Robert D Nowak"
        ],
        "title": "LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning",
        "volume": "01",
        "id": "7",
        "page": "43",
        "year": "2024"
    },
    {
        "abstract": "We present HAIR, the first dataset of expert-annotated historical aerial images covering different spatial regions spanning several decades. Historical aerial images are a treasure trove of insights into how the world has changed over the last hundred years. Understanding this change is especially important for investigating, among others, the impact of human development on biodiversity. The knowledge contained in these images, however, has not yet been fully unlocked, as this requires semantic segmentation models that are optimized for this type of data. Current models are developed for modern color images, and they do not perform well in historical data that is typically in grayscale. Furthermore, there is no benchmark historical grayscale aerial data that can be used to develop specific segmentation models for it. We here assess the issues of using semantic segmentation models designed for modern color images in historic grayscale data, and introduce HAIR as the first benchmark dataset of large-scale historical aerial grayscale images. HAIR contains ~9*10^9 pixels of high-resolution aerial land images covering the years within the period 1947 - 1998, with detailed annotations performed by domain experts. By using HAIR, we show that pre-training on modern satellite images converted to grayscale does not improve the performance compared to training only on historic aerial grayscale data, stressing the relevance of using actual historical and grayscale aerial data for these studies. We further show that state-of-the-art models underperform when trained on grayscale data compared to using the same data in color, and discuss the challenges faced by these models when applied directly to aerial grayscale data. Overall, HAIR appears as a powerful tool to aid in developing segmentation models that are able to extract the rich and valuable information from historical grayscale images.",
        "authors": [
            "Saeid Shamsaliei", "Odd Erik Gundersen", "Knut Tore Alfredsen", "Jo Halvard Halleraker", "Anders Foldvik"
        ],
        "title": "Highlighting Challenges of State-of-the-Art Semantic Segmentation with HAIR - A Dataset of Historical Aerial Images",
        "volume": "01",
        "id": "8",
        "year": "2024",
        "page":"31"
    },
    {
        "abstract": "The plant kingdom exhibits remarkable diversity that must be maintained for global ecosystem sustainability. However, plant life is currently disproportionately disappearing at a rapid rate, putting many essential functions---such as ecosystem production, resistance, and resilience---at risk. Plant specimen identification---the first step of plant biodiversity research---is heavily bottlenecked by a shortage of qualified experts. The botanical community has imaged large volumes of annotated physical herbarium specimens, which present a huge potential for building artificial intelligence systems that can assist researchers. In this paper, we present a novel large--scale, fine--grained dataset, NAFlora-1M, which consists of 1,050,182 hebarium images covering 15,501 North American vascular plant species (90% of the known species). Addressing gaps from previous research efforts, NAFlora-1M is the first–ever dataset to closely replicate the real--world task of herbarium specimen identification, as the dataset is intended to cover as many of the taxa in North America as possible. We highlight some key characteristics of NAFlora-1M from a machine learning dataset perspective: high--quality labels rigorously peer--reviewed by experts; hierarchical class structure; long–tailed and imbalanced class distribution; high image resolution; and extensive image quality control for consistent scale and color. In addition, we present several baseline models, along with benchmarking results from a Kaggle competition: A total of 134 teams benchmarked the dataset in a total of 1,663 submissions; the leading team achieved an 87.66% macro-F score with a 1–billion–parameter ensemble model---leaving substantial room for future improvement in both performance and efficiency. We believe that NAFlora-1M is an excellent starting point to encourage the development of botanical AI applications, thereby facilitating enhanced monitoring of plant diversity and conservation efforts. The dataset and training scripts are available at https://github.com/dpl10/NAFlora-1M.",
        "authors": [
            "John Park", "Riccardo de Lutio", "Brendan Rappazzo", "Barbara Ambrose", "Fabian Michelangeli", "Kimberly Watson", "Serge Belongie", "Damon Little" 
        ],
        "title": "NAFlora-1M: Continental-Scale High-Resolution Fine-Grained Plant Classification Dataset",
        "volume": "01",
        "id": "9",
        "year": "2024",
        "page":"21"
    },
    {
        "abstract": "Pseudo-labeling is a popular semi-supervised learning technique to leverage unlabeled data when labeled samples are scarce. The generation and selection of pseudo-labels heavily rely on labeled data. Existing approaches implicitly assume that the labeled data is gold standard and “perfect”. However, this can be violated in reality with issues such as mislabeling or ambiguity. We address this overlooked aspect and show the importance of investigating labeled data quality to improve any pseudo-labeling method. Specifically, we introduce a novel data characterization and selection framework called DIPS to extend pseudo-labeling. We select useful labeled and pseudo-labeled samples via analysis of learning dynamics. We demonstrate the applicability and impact of DIPS for various pseudo-labeling methods across an extensive range of real-world tabular and image datasets. Additionally, DIPS improves data efficiency and reduces the performance distinctions between different pseudo-labelers. Overall, we highlight the significant benefits of a data-centric rethinking of pseudo-labeling in real-world settings.",
        "authors": [
            "Nabeel Seedat", "Nicolas Huynh", "Fergus Imrie", "Mihaela van der Schaar"
        ],
        "title": "You can't handle the (dirty) truth: Data-centric Insights Improve Pseudo-Labeling",
        "volume": "01",
        "id": "10",
        "year": "2024",
        "page":"21"
    },
    {
        "abstract": "Molecular representation learning (MRL) is a powerful contribution by machine learning tochemistry as it converts molecules into numerical representations, which is fundamental fordiverse downstream applications, such as property prediction and drug design. While MRLhas had great success with proteins and general biomolecules, it has yet to be exploredfor carbohydrates in the growing fields of glycoscience and glycomaterials (the study anddesign of carbohydrates). This under-exploration can be primarily attributed to the limitedavailability of comprehensive and well-curated carbohydrate-specific datasets and a lackof machine learning (ML) techniques tailored to meet the unique problems presentedby carbohydrate data. Interpreting and annotating carbohydrate data is generally morecomplicated than protein data and requires substantial domain knowledge. In addition,existing MRL methods were predominately optimized for proteins and small biomoleculesand may not be effective for carbohydrate applications without special modifications. Toaddress this challenge, accelerate progress in glycoscience and glycomaterials, and enrichthe data resources of the ML community, we introduce GlycoNMR. GlycoNMR containstwo laboriously curated datasets with 2,609 carbohydrate structures and 211,543 annotatednuclear magnetic resonance (NMR) atomic-level chemical shifts that can be used to trainML models for precise atomic-level prediction. NMR data is one of the most appealingstarting points for developing ML techniques to facilitate glycoscience and glycomaterialsresearch, as NMR is the preeminent technique in carbohydrate structure research, andbiomolecule structure is among the foremost predictors of functions and properties. Wetailored a set of carbohydrate-specific features and adapted existing 3D-based graph neuralnetworks to tackle the problem of predicting NMR shifts effectively. For illustration, webenchmark these modified MRL models on GlycoNMR.",
        "authors": [
            "Zizhang Chen", "Ryan Paul Badman", "Bethany Lachele Foley", "Robert J Woods", "Pengyu Hong" 
        ],
        "title": "GlycoNMR: Dataset and Benchmark of Carbohydrate-Specific NMR Chemical Shift for Machine Learning Research",
        "volume": "01",
        "id": "11",
        "year": "2024",
        "page":"37"
    },
    {
        "abstract": "This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experiments with over 50000 CPU and 800 GPU hours of computations, we evaluate and compare the performance of these baseline algorithms on the collected datasets, offering insights into their strengths, limitations, and potential areas of improvement. Our benchmarking framework serves as a valuable resource for researchers and practitioners, facilitating the development of more robust and reliable offline safe RL solutions in safety-critical applications. The benchmark website is available at www.offline-saferl.org.",
        "authors": [
            "Zuxin Liu", "Zijian Guo", "Haohong Lin", "Yihang Yao", "Jiacheng Zhu", "Zhepeng Cen", "Hanjiang Hu", "Wenhao Yu", "Tingnan Zhang", "Jie Tan", "Ding Zhao" 
        ],
        "title": "Datasets and Benchmarks for Offline Safe Reinforcement Learning",
        "volume": "01",
        "id": "12",
        "year": "2024",
        "page":"29"
    },
]
