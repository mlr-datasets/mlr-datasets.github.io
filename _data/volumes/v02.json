[
    {
      "abstract": "In today’s data-driven world, addressing bias is essential to minimize discriminatory outcomes and work toward fairness in machine learning models. This paper presents a novel data-centric framework for bias analysis, harnessing the power of counterfactual reasoning. We detail a process for generating plausible counterfactuals suited for group evaluation, using probabilistic distributions and optionally incorporating domain knowledge, as a more efficient alternative to computationally intensive generative models.Additionally, we introduce the Counterfactual Confusion Matrix, from which we derive a suite of metrics that provide a comprehensive view of a model's behaviour under counterfactual conditions. These metrics offer unique insights into the model's resilience and susceptibility to changes in sensitive attributes, such as sex or race. We demonstrate their utility and complementarity with standard group fairness metrics through experiments on real-world datasets. Our results show that domain knowledge is key, and that our metrics can reveal subtle biases that traditional bias evaluation strategies may overlook, providing a more nuanced understanding of potential model bias.",
      "authors": [
            "Mariana Pinto", "Andre V Carreiro", "Pedro Madeira", "Alberto Lopez", "Hugo Gamboa" 
        ],
        "title": "The Matrix Reloaded: Towards Counterfactual Group Fairness in Machine Learning",
        "volume": "02",
        "id": "1",
        "year": "2024",
        "page":"55"
    },
    {
      "abstract": "In the consumer lending market, women tend to have lower access to credit than men, despite evidence suggesting that women are better at repaying their debts. This study explores the potential impact of leveraging alternative data, which traditionally has not been used by financial institutions, on credit risk predictions between men and women. By leveraging unique data on individuals' credit card default behaviors and their purchase behaviors at a supermarket, we simulate a credit card issuer's credit scoring process. In the absence of supermarket data, the algorithm's predictive accuracy for women is about 2.3% lower than that for men. We then integrate data from each of the 410 product markets within the supermarket into the algorithm and measure the changes in the gender gap in predictive accuracy. We find a wide variation in both direction and magnitude in the incremental gender gap, ranging from -142% to 70% compared to the baseline. These findings highlight that leveraging alternative data from a non-financial domain can lead to fairer credit outcomes, but only under certain conditions. We characterize the conditions by identifying two data properties: the capacity to proxy gender and the relative amount of creditworthiness signals data provide for each gender.", 
      "authors": [
            "Jung Youn Lee", "Joonhyuk Yang" 
        ],
        "title": "Properties of Alternative Data for Fairer Credit Risk Predictions",
        "volume": "02",
        "id": "2",
        "year": "2024",
        "page":"27"
    },
    {
       "abstract": "Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and scope. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate and standardized evaluation of OOD detection methodologies at large scale. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale data sets (ImageNet) and foundation models (e.g., CLIP and DINOv2), and expands its scope to investigate full-spectrum OOD detection which considers semantic and covariate distribution shifts at the same time. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool of OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims to drive advancements and offer a more robust and comprehensive evaluation benchmark for OOD detection research.", 
       "authors": [
            "Jingyang Zhang", "Jingkang Yang", "Pengyun Wang", "Haoqi Wang", "Yueqian Lin", "Haoran Zhang", "Yiyou Sun", "Xuefeng Du", "Yixuan Li", "Ziwei Liu", "Yiran Chen", "Hai Li"
        ],
        "title": "OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection",
        "volume": "02",
        "id": "3",
        "page": "32",
        "year": "2024"
    },
    {
       "abstract": "As large models become increasingly prevalent, watermarking has emerged as a crucial technology for copyright protection, authenticity verification, and content tracking. The rise of multimodal applications further amplifies the importance of effective watermarking techniques. While watermark robustness is critical for real-world deployment, the current understanding of watermark robustness against various forms of corruption remains limited. Our study evaluates watermark robustness in both image and text domains, testing against an extensive set of 100 image perturbations and 63 text perturbations. The results reveal significant vulnerabilities in contemporary watermarking approaches - detection accuracy deteriorates by more than 50\% under common perturbations, highlighting a critical gap between current capabilities and practical requirements. These findings emphasize the urgent need for more robust watermarking methods that can withstand real-world disturbances. Our project website can be found at https://mmwatermark-robustness.github.io/.", 
       "authors": [
            "Jielin Qiu", "William Han", "Xuandong Zhao", "Shangbang Long", "Christos Faloutsos", "Lei Li" 
        ],
        "title": "Evaluating Durability: Benchmark Insights into Image and Text Watermarking",
        "volume": "02",
        "id": "4",
        "page": "44",
        "year": "2024"
    },
    {
       "abstract": "Personal assistants, automatic speech recognizers and dialogue understanding systems are becoming more critical in our interconnected digital world. A clear example is air traffic control (ATC) communications. ATC aims at guiding aircraft and controlling the airspace in a safe and optimal manner. These voice-based dialogues are carried between an air traffic controller (ATCO) and pilots via very-high frequency radio channels. In order to incorporate these novel technologies into ATC, large-scale annotated datasets are required to develop the data-driven AI systems. Two examples are automatic speech recognition (ASR) and natural language understanding (NLU).However, ATC is considered a low-resource domain. In this paper, we introduce the ATCO2 corpus, a dataset that aims at fostering research on the challenging ATC field, which has lagged behind due to lack of annotated data. In addition, we also open-source a GitHub repository that contains data preparation and training scripts useful to replicate our baselines related to ASR and NLU.The ATCO2 corpus covers 1) audio and radar data collection and pre-processing, 2) pseudo-transcriptions of speech audio, and 3) extraction of ATC-related named entities. The ATCO2 corpus is split into three subsets: (i) ATCO2-test-set corpus contains 4 hours of ATC speech with manual transcripts and a subset with gold transcriptions for named-entity recognition (callsign, command, value) and speaker role detection. (ii) The ATCO2-test-set-1h corpus is a one-hour open-sourced subset from the 4h test set.\footnote{Free to download, available at:  https://www.atco2.org/data. (iii) The ATCO2-PL-set corpus consists of 5'281 hours of pseudo-transcribed ATC speech enriched with contextual information (list of relevant n-gram sequences per utterance), speaker turn information, signal-to-noise ratio estimate and English language detection score per sample. The whole ATCO2 corpus is publicly distributed through ELDA catalog (https://catalog.elra.info/en-us/repository/browse/ELRA-S0484/). We expect the corpus will foster research on robust ASR and NLU not only in the field of ATC communications but also in the general research community.",
       "authors": [
            "Juan Pablo Zuluaga Gomez", "Karel Veselý", "Igor Szöke", "Alexander Blatt", "Petr Motlicek", "Martin Kocour", "Khalid Choukri", "Iuliia Nigmatulina", "Claudia Cevenini", "Allan Tart", "Jan Cernocký", "Dietrich Klakow"
        ],
        "title": "ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications",
        "volume": "02",
        "id": "5",
        "page": "45",
        "year": "2024"
    },
    {
       "abstract": "When assessing the quality of prediction models in machine learning, confidence intervals (CIs) for the generalization error, which measures predictive performance, are a crucial tool. Luckily, there exist many methods for computing such CIs and new promising approaches are continuously being proposed. Typically, these methods combine various resampling procedures, most popular among them cross-validation and bootstrapping, with different variance estimation techniques. Unfortunately, however, there is currently no consensus on when any of these combinations may be most reliably employed and how they generally compare. In this work, we conduct a large-scale study comparing CIs for the generalization error, the first one of such size, where we empirically evaluate 13 different CI methods on a total of 19 tabular regression and classification problems, using seven different inducers and a total of eight loss functions.We give an overview of the methodological foundations and inherent challenges of constructing CIs for the generalization error and provide a concise review of all 13 methods in a unified framework. Finally, the CI methods are evaluated in terms of their relative coverage frequency, width, and runtime. Based on these findings, we can identify a subset of methods that we would recommend.We also publish the datasets as a benchmarking suite on OpenML and our code on GitHub to serve as a basis for further studies.",
       "authors": [
            "Hannah Schulz-Kümpel", "Sebastian Felix Fischer", "Roman Hornung", "Anne-Laure Boulesteix", "Thomas Nagler", "Bernd Bischl" 
        ],
        "title": "Constructing Confidence Intervals for “the” Generalization Error – a Comprehensive Benchmark Study",
        "volume": "02",
        "id": "6",
        "page": "73",
        "year": "2025"
    },
    {
       "abstract": "The conclusion of an AI challenge is not the end of its lifecycle; ensuring a long-lasting impact requires meticulous post-challenge activities. The long-lasting impact also needs to be organised. This chapter covers the various activities after the challenge is formally finished. This work identifies target audiences for post-challenge initiatives and outlines methods for collecting and organizing challenge outputs. The multiple outputs of the challenge are listed, along with the means to collect them. The central part of the chapter is a template for a typical post-challenge paper, including possible graphs and advice on how to turn the challenge into a long-lasting benchmark.",
       "authors": [
            "David Rousseau", "Antoine Marot", "Zhen Xu" 
        ],
        "title": "Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions",
        "volume": "02",
        "id": "7",
        "page": "20",
        "year": "2025"
    },
    {
       "abstract": "Super-resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts in weather, climate, and related areas. However, the lack of standardized benchmark datasets for comparing and validating SR methods hinders progress and adoption in SciML. To address this, we introduce SuperBench (https://github.com/erichson/SuperBench), the first benchmark dataset featuring high-resolution datasets (up to  dimensions), including data from fluid flows, cosmology, and weather. Here, we focus on validating spatial SR performance from data-centric and physics-preserved perspectives, as well as assessing robustness to data degradation tasks. While deep learning-based SR methods (developed in the computer vision community) excel on certain tasks, despite relatively limited prior physics information, we identify limitations of these methods in accurately capturing intricate fine-scale features and preserving fundamental physical properties and constraints in scientific data. These shortcomings highlight the importance and subtlety of incorporating domain knowledge into ML models. We anticipate that SuperBench will help to advance SR methods for science.",
        "authors": [
             "Pu Ren", "N. Benjamin Erichson", "Junyi Guo", "Shashank Subramanian", "Omer San", "Zarija Lukic", "Michael W. Mahoney"
        ],
        "title": "SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning",
        "volume": "02",
        "id": "8",
        "page": "45",
        "year": "2025"
    },
    {
       "abstract": "Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to cap- ture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the diagnostic visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Train, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Train provides a platform for investigating a wide range of visual logical learning challenges. We evaluate a variety of AI systems including traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Our evaluations demonstrate that even SOTA AI faces difficulties in dealing with visual logical learning challenges, highlighting unique advantages and limitations of each methodology. Overall, V-LoL opens up new avenues for understanding and enhancing current abilities in visual logical learning for AI systems.",
       "authors": [
            "Lukas Helff", "Wolfgang Stammer", "Hikaru Shindo", "Devendra Singh Dhami", "Kristian Kersting" 
        ],
        "title": "V-LoL: A Diagnostic Dataset for Visual Logical Learning",
        "volume": "02",
        "id": "9",
        "page": "41",
        "year": "2025"
    },
    {
       "abstract": "This document serves as a comprehensive guide for designing and organizing effective challenges, particularly within the domains of machine learning and artificial intelligence. It provides detailed guidelines on every phase of the process, from conception and execution to post-challenge analysis. Challenges function as motivational mechanisms that drive participants to address significant tasks. Consequently, organizers must establish rules that fulfill objectives beyond mere participant engagement. These objectives include solving real-world problems, advancing scientific or technical fields, facilitating discoveries, educating the public, providing platforms for skill development, and recruiting new talent. The creation of a challenge is analogous to product development; it requires enthusiasm, rigorous testing, and aims to attract participants. The process commences with a comprehensive plan, such as a challenge proposal submitted for peer review at an international conference. This document presents guidelines for developing such a robust challenge plan, ensuring it is both engaging and impactful.",
       "authors": [
            "Hugo Jair Escalante", "Isabelle Guyon", "Addison Howard", "Walter Reade", "Sebastien Treguer" 
        ],
        "title": "Challenge design roadmap",
        "volume": "02",
        "id": "10",
        "page": "42",
        "year": "2025"
    },
    {
       "abstract": "As Machine Learning (ML) systems continue to grow, the demand for relevant and comprehensive datasets becomes imperative. There is limited study on the challenges of data acquisition due to ad-hoc processes and lack of consistent methodologies. We first present an investigation of current data marketplaces, revealing lack of platforms offering detailed information about datasets, transparent pricing, standardized data formats. With the objective of inciting participation from the data-centric AI community, we then introduce the DAM challenge, a benchmark to model the interaction between the data providers and acquirers in a data marketplace. The benchmark was released as a part of DataPerf Mazumder et al. (2022). Our evaluation of the submitted strategies underlines the need for effective data acquisition strategies in ML.",
        "authors": [
            "Lingjiao Chen", "Bilge Acun", "Newsha Ardalani", "Yifan Sun", "Feiyang Kang", "Hanrui Lyu", "Yongchan Kwon", "Ruoxi Jia", "Carole-Jean Wu", "Matei Zaharia", "James Zou"
        ],
        "title": "Data Acquisition: A New Frontier in Data-centric AI",
        "volume": "02",
        "id": "11",
        "page": "19",
        "year": "2025"
    },
    {
       "abstract": "Infectious disease diagnostics primarily rely on physicians' clinical expertise and rapid antigen/antibody tests, a subjective approach prone to errors due to various factors including patient history accuracy and physician experience. To address these challenges, we propose a biological evidence-based diagnostic tool using deep learning to analyze patient-derived single-cell RNA sequencing (scRNA-seq) profiles from blood samples. scRNA-seq provides high-resolution gene expression data at the single-cell level, capturing unique transcriptional signatures and immunological responses induced by different viral infections. In this work, we conducted the first-of-its-kind benchmark study to evaluate five computational models, including four deep learning-based methods (contrastiveVI, scVI, SAVER, scGPT) and PCA as a baseline - trained and evaluated on patient-derived scRNA-seq datasets carefully sourced by us. We assess their efficacy in distinguishing scRNA-seq profiles associated with various viral infections, aiming to identify distinct immunological features representative of each infection. The results demonstrate that contrastiveVI, outperforms other models in all key performance metrics and the visual cluster performance. Furthermore, our research also underscores the substantial influence of batch effects when analyzing scRNA-seq data from multiple sources. Overall, our study successfully demonstrates that deep learning models can accurately identify the type of infection from patient plasma samples based on scRNA-seq profiles, and improve the accuracy and specificity in the diagnosis of infectious diseases. This research contributes to the development of more objective, evidence-based diagnostic methods in the infectious disease domain, potentially reducing diagnostic errors and improving patient outcomes.",
       "authors": [
            "Ziwei Yang", "Xuxi Chen", "Biqing Zhu", "Tianlong Chen", "Zhangyang Wang" 
        ],
        "title": "Deep Learning for Accurate Diagnosis of Viral Infections through scRNA-seq Analysis: A Comprehensive Benchmark Study",
        "volume": "02",
        "id": "12",
        "page": "19",
        "year": "2025"
    },
    {
       "abstract": "In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text quality in large unlabelled NLP datasets in a model agnostic manner to assign the text instances a quality score.By proposing the text quality metric, the paper establishes a framework to identify and eliminate low-quality text instances, leading to improved training efficiency for LM models. Experimental results over multiple models and datasets demonstrate the efficacy of this approach, showcasing substantial gains in training effectiveness and highlighting the potential for resource-efficient LM training.For example, we observe an absolute accuracy improvement of 0.9% averaged over 14 downstream evaluation tasks for multiple LM models while using 40% lesser data and training 42% faster when training on the OpenWebText dataset and 0.8% average absolute accuracy improvement while using 20% lesser data and training 21% faster on the Wikipedia dataset.",
        "authors": [
            "Vasu Sharma", "Karthik Padthe", "Newsha Ardalani", "Kushal Tirumala", "Russell Howes", "Hu Xu", "Po-Yao Huang", "Daniel Li Chen", "Armen Aghajanyan", "Gargi Ghosh", "Luke Zettlemoyer"
        ],
        "title": "Text Quality-Based Pruning for Efficient Training of Language Models",
        "volume": "02",
        "id": "13",
        "page": "13",
        "year": "2025"
    },
    {
       "abstract": "The conclusion of an AI challenge is not the end of its lifecycle; ensuring a long-lasting impact requires meticulous post-challenge activities. The long-lasting impact also needs to be organised. This chapter covers the various activities after the challenge is formally finished. This work identifies target audiences for post-challenge initiatives and outlines methods for collecting and organizing challenge outputs. The multiple outputs of the challenge are listed, along with the means to collect them. The central part of the chapter is a template for a typical post-challenge paper, including possible graphs and advice on how to turn the challenge into a long-lasting benchmark.",
       "authors": [
            "David Rousseau", "Antoine Marot", "Zhen Xu" 
        ],
        "title": "Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions",
        "volume": "02",
        "id": "14",
        "page": "20",
        "year": "2025"
    },
    {
       "abstract": "The conclusion of an AI challenge is not the end of its lifecycle; ensuring a long-lasting impact requires meticulous post-challenge activities. The long-lasting impact also needs to be organised. This chapter covers the various activities after the challenge is formally finished. This work identifies target audiences for post-challenge initiatives and outlines methods for collecting and organizing challenge outputs. The multiple outputs of the challenge are listed, along with the means to collect them. The central part of the chapter is a template for a typical post-challenge paper, including possible graphs and advice on how to turn the challenge into a long-lasting benchmark.",
       "authors": [
            "David Rousseau", "Antoine Marot", "Zhen Xu" 
        ],
        "title": "Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions",
        "volume": "02",
        "id": "15",
        "page": "20",
        "year": "2025"
    },
    {
       "abstract": "The conclusion of an AI challenge is not the end of its lifecycle; ensuring a long-lasting impact requires meticulous post-challenge activities. The long-lasting impact also needs to be organised. This chapter covers the various activities after the challenge is formally finished. This work identifies target audiences for post-challenge initiatives and outlines methods for collecting and organizing challenge outputs. The multiple outputs of the challenge are listed, along with the means to collect them. The central part of the chapter is a template for a typical post-challenge paper, including possible graphs and advice on how to turn the challenge into a long-lasting benchmark.",
       "authors": [
            "David Rousseau", "Antoine Marot", "Zhen Xu" 
        ],
        "title": "Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions",
        "volume": "02",
        "id": "16",
        "page": "20",
        "year": "2025"
    },
    {
       "abstract": "The conclusion of an AI challenge is not the end of its lifecycle; ensuring a long-lasting impact requires meticulous post-challenge activities. The long-lasting impact also needs to be organised. This chapter covers the various activities after the challenge is formally finished. This work identifies target audiences for post-challenge initiatives and outlines methods for collecting and organizing challenge outputs. The multiple outputs of the challenge are listed, along with the means to collect them. The central part of the chapter is a template for a typical post-challenge paper, including possible graphs and advice on how to turn the challenge into a long-lasting benchmark.",
       "authors": [
            "David Rousseau", "Antoine Marot", "Zhen Xu" 
        ],
        "title": "Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions",
        "volume": "02",
        "id": "17",
        "page": "20",
        "year": "2025"
    },
    {
       "abstract": "The conclusion of an AI challenge is not the end of its lifecycle; ensuring a long-lasting impact requires meticulous post-challenge activities. The long-lasting impact also needs to be organised. This chapter covers the various activities after the challenge is formally finished. This work identifies target audiences for post-challenge initiatives and outlines methods for collecting and organizing challenge outputs. The multiple outputs of the challenge are listed, along with the means to collect them. The central part of the chapter is a template for a typical post-challenge paper, including possible graphs and advice on how to turn the challenge into a long-lasting benchmark.",
       "authors": [
            "David Rousseau", "Antoine Marot", "Zhen Xu" 
        ],
        "title": "Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions",
        "volume": "02",
        "id": "18",
        "page": "20",
        "year": "2025"
    }
]
